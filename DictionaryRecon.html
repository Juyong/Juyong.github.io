<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>DictionaryLearning Reconstruction</title>
<style type="text/css">
.ys01 {
	font-size: 30px;
	font-family: Arial, Helvetica, sans-serif;
	font-weight: bold;
	color: #57524C;
}
.ys02 {
	font-family: Arial, Helvetica, sans-serif;
	font-weight: bold;
	color: #6293A2;
	text-align: center;
}
.ys2 {
}
.ys2 {
	font-weight: bold;
	font-family: Arial, Helvetica, sans-serif;
	font-size: 16px;
	color: #6293A2;
	text-align: center;
}
.ys2 sup {
	color: #57524C;
}
.ys3 {
	font-weight: bold;
}
.ys3 {
	font-family: Arial, Helvetica, sans-serif;
}
.ys3 {
	text-align: center;
	color: #57524C;
}
.ys3 {
}
.ys4 {
	text-align: center;
}
.ys2 .ys2 {
	color: #57524C;
}
.ys5 {
	font-weight: bold;
	font-family: Arial, Helvetica, sans-serif;
}
.ys6 {
	font-family: Arial, Helvetica, sans-serif;
}
.ys7 {
	text-align: center;
}
a:link {
	text-decoration: none;
}
a:visited {
	text-decoration: none;
}
a:hover {
	text-decoration: underline;
}
a:active {
	text-decoration: none;
}
.teaser {
	font-weight: bold;
	font-family: "Times New Roman", Times, serif;
	color: #57524C;
	text-align: left;
}
.text {
	font-family: "Times New Roman", Times, serif;
	color: #57524C;
	text-align: justify;
	font-weight: normal;
}
.title1 {
	font-family: Arial, Helvetica, sans-serif;
	color: #6293A2;
	font-weight: bold;
	text-align: left;
	font-size: 18px;
}
.text2 {
	text-align: justify;
	font-family: "Times New Roman", Times, serif;
	color: #57524C;
	font-weight: normal;
}
.title1 .teaser {
	text-align: justify;
}
.copyright {
	font-family: "Times New Roman", Times, serif;
	color: #57524C;
	text-align: right;
}
.copy {
	font-family: "Times New Roman", Times, serif;
	text-align: right;
	color: #57524C;
}
.download {
}
.download {
	font-family: Arial, Helvetica, sans-serif;
}
.download {
	font-weight: bold;
}
.download {
	color: #6293A2;
}
body {
	background-color: #DDDDEB;
	text-align: center;
}
.teaser {
	text-align: justify;
}
.copy1 {
	text-align: right;
	color: #000;
	font-family: Arial, Helvetica, sans-serif;
	font-size: 14px;
}
</style>
</head>

<body>
<table width="1044" height="2915" align="center" background="photos/test3.jpg">
  <tr>
    <td width="1044" height="2909" align="center" valign="top"><p>&nbsp;</p>
      <table width="900">
      <tr>
        <td><p align="center" class="ys01">Robust Surface Reconstruction via Dictionary Learning</p>
          <p class="ys2">Shiyao Xiong<sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a href="http://staff.ustc.edu.cn/~juyong/" target="_new">Juyong Zhang</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a href="http://www.ntu.edu.sg/home/asjmzheng/" target="_new">Jianmin Zheng</a><sup>2</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a href="http://www.ntu.edu.sg/home/asjfcai/" target="_new">Jianfei Cai</a><sup>2</sup>&nbsp; &nbsp;&nbsp; <a href="http://staff.ustc.edu.cn/~lgliu/" target="_new">&nbsp;Ligang Liu</a><sup>1</sup></p>
          <p class="ys2"><sup>1</sup><a href="http://www.ustc.edu.cn/" target="_new">University of Science and Technology of China</a>&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; <sup>2</sup><a href="http://www.ntu.edu.sg/Pages/home.aspx" target="_new">Nanyang Technological University</a></p>
          <p class="ys2">ACM Transactions on Graphics 2014<br />
          (Proc.<a href="http://sa2014.siggraph.org/simp/" target="_new">SIGGRAPH ASIA 2014</a>) </p></td>
      </tr>
      <tr>
        <td><p class="ys02">&nbsp;</p>
          <p class="ys02"><img src="photos/face_teaser.jpg" width="880" height="464" /></p></td>
      </tr>
      <tr>
        <td><p class="teaser">Teaser:<span class="text"> In our proposed reconstruction method, both the vertex positions and connectivity are improved as iterations increase  (from left to right).</span></p></td>
      </tr>
      <tr>
        <td>&nbsp;</td>
      </tr>
      <tr>
        <td><p class="title1">Abstract:</p></td>
      </tr>
      <tr>
        <td><p class="text2">Surface reconstruction from point cloud is of great practical importance in computer graphics. Existing methods often realize recon-struction via a few phases with respective goals, whose integration may not give an optimal solution.In this paper,to avoid the inherent limitations of multi-phase processing in the prior art, we propose a uniﬁed framework that treats geometry and connectivity construc-tion as one joint optimization problem. The framework is based on dictionary learning in which the dictionary consists of the vertices of the reconstructed triangular mesh and the sparse coding matrix encodes the connectivity of the mesh. The dictionary learning is formulated as a constrained l<sub>2,q</sub>-optimization (0&lt;q&lt;1) , aiming to ﬁnd the vertex position and triangulation that minimize an energy function composed of point-to-mesh metric and regularization. Our formulation takes many factors into account within the same framework, including distance metric, noise/outlier resilience, sharp fea-ture preservation, no need to estimate normal, etc., thus providing a global and robust algorithm that is able to efﬁciently recover a piecewise smooth surface from dense data points with imperfec-tions. Extensive experiments using synthetic models, real world models, and publicly available benchmark show that our method improves the performance of the state-of-the-art in terms of accu-racy, robustness to noise and outliers, geometric feature and detail preservation, and mesh connectivity.</p></td>
      </tr>
      <tr>
        <td>&nbsp;</td>
      </tr>
      <tr>
        <td><p class="title1">Results:</p></td>
      </tr>
      <tr>
        <td><span class="ys02"><img src="photos/merlion_compare_size3.jpg" alt="" width="880" height="450" /></span></td>
      </tr>
      <tr>
        <td><span class="teaser">Figure 1:</span><span class="text">Reconstruction of the Merlion model by our proposed method and state-of-the-art: SP [Kazhdan and Hoppe 2013], APSS [Guen-nebaud and Gross 2007], RIMLS [ ¨Oztireli et al. 2009], and SC [Dey and Wang 2013]. It can be seen that our reconstruction can recover different levels of details better.</span></td>
      </tr>
      <tr>
        <td>&nbsp;</td>
      </tr>
      <tr>
        <td><span class="ys02"><img src="photos/cubenoise4.jpg" width="880" height="160" /></span></td>
      </tr>
      <tr>
        <td><span class="teaser">Figure 2:</span><span class="text"> <span class="text2">Zoom-in results of the cube model with 4\% noise using different reconstruction methods. The EAR algorithm is used to resample the input point cloud for the four comparative methods. The second column visualizes the normal produced by EAR.</span></span></td>
      </tr>
      <tr>
        <td>&nbsp;</td>
      </tr>
      <tr>
        <td><img src="photos/chair.jpg" width="880" height="250" align="top" /></td>
      </tr>
      <tr>
        <td><span class="teaser">Figure 3: </span><span class="text2">Various reconstruction results of the chair model. The point cloud is scanned by Kinect. The EAR algorithm is used to resample the noisy input into a clean point set with reliable normals.</span></td>
      </tr>
      <tr>
        <td>&nbsp;</td>
      </tr>
      <tr>
        <td><img src="photos/fifa.jpg" width="870" height="150" /></td>
      </tr>
      <tr>
        <td><span class="teaser">Figure 4: </span><span class="text2">Various reconstruction results of the FIFA model. The point cloud is scanned by a laser scanner. Note that here we use locally weighted PCA instead of EAR to estimate normals for the comparative methods. This is to demonstrate that the inferior performance of the comparative methods is not due to the inaccurate normal estimation of a particular preprocessing method.</span></td>
      </tr>
      <tr>
        <td>&nbsp;</td>
      </tr>
      <tr>
        <td><span class="title1">Acknowledgements:</span></td>
      </tr>
      <tr>
        <td height="110"><p class="text2">The authors thank Hao (Richard) Zhang, Daniel Cohen-Or and the reviewers for their valuable comments. We also thank Hao Li for his help to prepare the video. This work was supported by the NSF of China (Nos. 61303148, 61222206), NSF of AnHui Province, China (No. 1408085QF119), Specialized Research Fund for the Doctoral Program of Higher Education under contract (No. 20133402120002), One Hundred Talent Project of the Chinese Academy of Sciences, Singapore MoE AcRF Tier-1 Grant RG30/11, Multi-plAtform Game Innovation Centre (MAGIC) and BeingThere Centre funded by the Singapore National Research Foundation under its IDM Futures Funding Initiative and administered by the Interactive &amp; Digital Media Programme Office, Media Development Authority.</p></td>
      </tr>
      <tr>
        <td>&nbsp;</td>
      </tr>
      <tr>
        <td><span class="title1">BibTex:</span></td>
      </tr>
      <tr>
        <td><span class="text2">@article{Xiong2014,<br />
author = {Shiyao Xiong and Juyong Zhang and Jianmin Zheng and Jianfei Cai and Ligang Liu},<br />
title = {Robust Surface Reconstruction via Dictionary Learning},<br />
journal = {ACM Transactions on Graphics (Proc. SIGGRAPH Aisa)},<br />
volume = {33},<br />
issue = {6},<br />
pages = {},<br />
year = {2014}<br />
} </span></td>
      </tr>
      <tr>
        <td>&nbsp;</td>
      </tr>
      <tr>
        <td><span class="title1">Downloads:</span></td>
      </tr>
      <tr>
        <td><span class="text2">Disclaimer: The paper listed on this page is copyright-protected. By clicking on the paper link below, you confirm that you or your institution have the right to access the corresponding pdf file.</span></td>
      </tr>
      <tr>
        <td><ul>
          <li class="download"><a href="Papers/Xiong.Reconstruction.2014.pdf">Paper</a></li>
          <li class="download"><a href="Papers/Demo_SigAsia14_Recon.mp4">Video</a></li>
          <li class="download"><a href="Papers/Reconstruction_Slides.pdf">Slides</a></li>
        </ul></td>
      </tr>
    </table>
      <table width="900" align="center">
        <tr>
          <td><hr /></td>
        </tr>
        <tr>
          <td class="copy1">Copyright &copy; 2014 <a href="http://gcl.ustc.edu.cn/" target="_new">GCL</a>&nbsp;, USTC</td>
        </tr>
      </table>
    <p>&nbsp;</p></td>
  </tr>
</table>
</body>
</html>
